{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `list_attr_celeba` Dataset\n",
    "A popular component of computer vision and deep learning revolves around identifying faces for various applications from logging into your phone with your face or searching through surveillance images for a particular suspect. This dataset is great for training and testing models for face detection, particularly for recognising facial attributes such as finding people with brown hair, are smiling, or wearing glasses. Images cover large pose variations, background clutter, diverse people, supported by a large quantity of images and rich annotations. This data was originally collected by researchers at MMLAB, The Chinese University of Hong Kong (specific reference in Acknowledgment section).\n",
    "\n",
    "\n",
    "\n",
    "- 202,599 number of face images of various celebrities\n",
    "- 10,177 unique identities, but names of identities are not given\n",
    "- 40 binary attribute annotations per image\n",
    "\n",
    "You can obtain the dataset from https://www.kaggle.com/jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, image\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Facial_keypoints.ipynb', 'GenderIDex.JPG', 'Transfer_learning.ipynb', 'img_align_celeba.zip', 'Week2-Class1-St.zip', 'weights.best.hdf5', '.DS_Store', 'Week2-Class1-St', 'test', 'weights-improvement-17-0.66.hdf5', 'weights-improvement-76-0.74.hdf5', 'weights-improvement-71-0.73.hdf5', 'input', 'small_c_d', 'weights-improvement-43-0.69.hdf5', 'weights-improvement-109-0.76.hdf5', 'celeb_small', 'weights-improvement-51-0.69.hdf5', 'weights-improvement-28-0.67.hdf5', 'weights-improvement-140-0.76.hdf5', 'Celeb_sets', 'P1_Facial_Keypoints', 'Gender_ID_VGG16.py', 'weights-improvement-57-0.71.hdf5', 'get-start-image-classification.ipynb', 'weights-improvement-79-0.75.hdf5', 'cats_dogs', 'list_attr_celeba.csv', 'test1.zip', 'train', 'weights-improvement-01-0.66.hdf5', 'model.h5', 'vgg16_1.h5', 'Gender_ID_VGG16_v02.py', '.ipynb_checkpoints', 'weights-improvement-41-0.69.hdf5', 'Gender_ID_Inception.py', 'diabetes.csv', 'Week2-Class1-Inst', 'weights-improvement-29-0.68.hdf5', 'weights-improvement-60-0.72.hdf5', 'weights-improvement-144-0.77.hdf5', 'train.zip', 'weights-improvement-56-0.70.hdf5', 'img_align_celeba']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',\n",
       "       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',\n",
       "       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair',\n",
       "       'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee',\n",
       "       'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male',\n",
       "       'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
       "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
       "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair',\n",
       "       'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
       "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mypath='/Users/gceran/Google Drive/Courses/MagniMind/Mentorship Program/DL-MentorProgram/DL_Mentor_Week2/Class 1'\n",
    "print(os.listdir(mypath))\n",
    "\n",
    "df=pd.read_csv(mypath+'/list_attr_celeba.csv')\n",
    "\n",
    "df.head()\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "400a293df3c8499059d9175f3915187074efd971"
   },
   "source": [
    "#### See sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build Model\n",
    "\n",
    "- First, copy VGG16 without the dense layers, use the weights from `imagenet`. Set the input shape to `(178,218,3)`.\n",
    "- Freeze the layers except the last two layers and print to see if the layers are trainable or not.\n",
    "- Build your sequential model (you are free to use a functioanl API as a further exercise). Include all the frozen VGG layers to your model. Add a Dense layer with 128 inouts and `relu` activation. Add a batch nomalizer, then a dense layer as the output layer. \n",
    "- Create an early stopping criteria monitorin the loss value for the validation set. Stop the search if the loss value deosnt change for two consecutive times.\n",
    "- Compile the model.\n",
    "- Save the best model automatically based on the performance of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation\n",
    "\n",
    "- Create a validation set with 20% of the data. Check the number of data points per class from both the train and validation sets.\n",
    "- Set your batch size to 20.\n",
    "- Create the data generator and set the `preprocessing_function` to `preprocess_input` of VGG16.\n",
    "- Create train and validation data generators (batches will be picked up from the dataframe). Set target size to (178,218) (you can try something else, but you need to do the corresponding change in the model).\n",
    "- Set your validation  and epoch step size (`validation_steps` and `steps_per_epoch`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "- Fit the model\n",
    "- save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
